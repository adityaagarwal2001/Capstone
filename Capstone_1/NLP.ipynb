{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "sns.set()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from CSV file and save in dataframe df\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\Adi\\Desktop\\Data_Science\\Capstone1\\DataSet_DataStory.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['result'] = (df.state == 'successful').astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-640ba492015b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'result'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "df = df[['name', 'result']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 95155 tokens\n",
      "['0', '00', '000', '000001', '00001', '0001', '000bp', '000km', '000mah', '001', '002', '003', '004', '005', '00tz', '01', '014352', '016', '02', '03', '030412', '04', '04y', '05', '054', '056', '06', '0625c', '068', '07', '08', '081', '086', '089', '09', '0blio', '0c370t', '0ct', '0f', '0gravity', '0h', '0kw', '0l', '0m', '0ne', '0x', '1', '10', '100', '1000', '10000', '100000', '1000000', '10000mah', '1000km', '1000mortel', '1000s', '1000timesyes', '1000v', '1000w', '1001', '10036', '100bracelets100days', '100cities', '100daysians', '100faces', '100giftcreations', '100k', '100lb', '100lbs', '100ma', '100pact', '100pg', '100s', '100shirts', '100th', '100yr', '101', '1017', '1018', '10197', '102', '10218', '10224', '10232', '1024', '10243', '10246', '10247', '10249', '10251', '10256', '103', '1031', '104', '1049', '105', '106', '1067', '107']\n"
     ]
    }
   ],
   "source": [
    "# Bag of words\n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Fill missing values in df.Position_Extra\n",
    "df.name.fillna('', inplace=True)\n",
    "\n",
    "# Instantiate the CountVectorizer: vec_alphanumeric\n",
    "vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# Fit to the data\n",
    "vec_alphanumeric.fit(df.name)\n",
    "\n",
    "# Print the number of tokens and first 15 tokens\n",
    "msg = \"There are {} tokens\"\n",
    "print(msg.format(len(vec_alphanumeric.get_feature_names())))\n",
    "print(vec_alphanumeric.get_feature_names()[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import Pipeline\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train test split\n",
    "X = df.name.str.lower()\n",
    "y = df.result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this pipeline and other techniques from datacamp in final modeling of Capstone1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on test data:  0.6616055061736935\n"
     ]
    }
   ],
   "source": [
    "# This pipeline method maybe used in full capstone1\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split out only the text data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=12, stratify=y)\n",
    "\n",
    "# Instantiate Pipeline object: pl\n",
    "pl = Pipeline([('vec', CountVectorizer()), ('clf', LogisticRegression())])\n",
    "\n",
    "# Fit to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on test data: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back to learning NLP\n",
    "# Convert whole X into cv\n",
    "cv = CountVectorizer()\n",
    "X_cv = cv.fit_transform(X)\n",
    "\n",
    "# Train test split\n",
    "X_traincv, X_testcv, y_train, y_test = train_test_split(X_cv, y, random_state=12, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260216, 140658)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_traincv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_traincv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6615709196555183"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_testcv, y_test) # really bad accuracy as countvector took all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF is a better method as it offsets a word if it occurs in most documents\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer(min_df=1, stop_words='english')\n",
    "\n",
    "# Convert whole X into cv\n",
    "X_tv = tv.fit_transform(X)\n",
    "\n",
    "# Train test split\n",
    "X_traintv, X_testtv, y_train, y_test = train_test_split(X_tv, y, random_state=12, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260216, 140363)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_traintv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.661144352598024"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_traintv, y_train)\n",
    "logreg.score(X_testtv, y_test) # really bad accuracy as countvector took all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes for prediction\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure target variable is integer type for NB\n",
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.fit(X_traintv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = mnb.predict(X_testtv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6516445889392315"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count= 0\n",
    "for i in range(len(prediction)):\n",
    "    if prediction[i] == np.array(y_test)[i]:\n",
    "        count += 1\n",
    "count/len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The models are pretty bad. This is because we tokenized words as regex alphanumeric\n",
    "# Let's use NLTK to tokenize important words and then run model\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "df['tokens'] = df.name.str.lower().apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Take small sample from df to save processing time\n",
    "\n",
    "data = df#.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words for tokens\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['filtered'] = data['tokens'].apply(lambda x: [w for w in x if w not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming - conver happiness, happy to happ\n",
    "# This may not be important for cap1, doing for learning purposes\n",
    "\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "#data['stem'] = data['filtered'].apply(lambda x: [ps.stem(w) for w in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets use new tokenizer which is unsupervized and comes pre trained. It can be trained as desired too\n",
    "\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Punkt tokenizer will not work for this problem. It creates too many custom tokens\n",
    "# It is better for full text instead of titles where it can tag words to noun, verb, etc.\n",
    "\n",
    "# refer to this youtube video\n",
    "# https://www.youtube.com/watch?v=6j6M2MtEqi8&index=4&list=PLQVvvaa0QuDf2JswnfiGkliBInZnIC4HL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Better to convert all text to lower case\n",
    "# Remove punctuation\n",
    "# Remove digits\n",
    "\n",
    "import string\n",
    "\n",
    "data.name = data.name.apply(lambda x: x.lower())\n",
    "punct = set(string.punctuation)\n",
    "data['filtered'] = data['filtered'].apply(lambda x: [w for w in x if w not in punct])\n",
    "\n",
    "digits = set(string.digits)\n",
    "data['filtered'] = data['filtered'].apply(lambda x: [w for w in x if w not in digits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tokens</th>\n",
       "      <th>filtered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the songs of adelaide &amp; abullah</td>\n",
       "      <td>[the, songs, of, adelaide, &amp;, abullah]</td>\n",
       "      <td>[songs, adelaide, abullah]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>greeting from earth: zgac arts capsule for et</td>\n",
       "      <td>[greeting, from, earth, :, zgac, arts, capsule...</td>\n",
       "      <td>[greeting, earth, zgac, arts, capsule, et]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>where is hank?</td>\n",
       "      <td>[where, is, hank, ?]</td>\n",
       "      <td>[hank]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>toshicapital rekordz needs help to complete album</td>\n",
       "      <td>[toshicapital, rekordz, needs, help, to, compl...</td>\n",
       "      <td>[toshicapital, rekordz, needs, help, complete,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>community film project: the art of neighborhoo...</td>\n",
       "      <td>[community, film, project, :, the, art, of, ne...</td>\n",
       "      <td>[community, film, project, art, neighborhood, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0                    the songs of adelaide & abullah   \n",
       "1      greeting from earth: zgac arts capsule for et   \n",
       "2                                     where is hank?   \n",
       "3  toshicapital rekordz needs help to complete album   \n",
       "4  community film project: the art of neighborhoo...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0             [the, songs, of, adelaide, &, abullah]   \n",
       "1  [greeting, from, earth, :, zgac, arts, capsule...   \n",
       "2                               [where, is, hank, ?]   \n",
       "3  [toshicapital, rekordz, needs, help, to, compl...   \n",
       "4  [community, film, project, :, the, art, of, ne...   \n",
       "\n",
       "                                            filtered  \n",
       "0                         [songs, adelaide, abullah]  \n",
       "1         [greeting, earth, zgac, arts, capsule, et]  \n",
       "2                                             [hank]  \n",
       "3  [toshicapital, rekordz, needs, help, complete,...  \n",
       "4  [community, film, project, art, neighborhood, ...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name\n",
    "# token = name tokenized as list of words\n",
    "# filered = tokens without stop words + punctuation removed + digits removed\n",
    "\n",
    "data[['name', 'tokens', 'filtered']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0\n",
      "0  0.500000\n",
      "1 -0.500000\n",
      "2 -0.500000\n",
      "3  0.500000\n",
      "4  0.408248\n"
     ]
    }
   ],
   "source": [
    "# Improve tokenization\n",
    "# Count vectorization is a bag of words technique\n",
    "# Hash vector is better because it reduced the dimensionality of super sparse matrix and speeds up computation\n",
    "# Remeber this is performed on original name column as tokenizer will convert it into tokens with low dimension\n",
    "# Cannot be used on filtered column because its already a list of tokens\n",
    "\n",
    "# Import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)' \n",
    "\n",
    "# Instantiate the HashingVectorizer: hashing_vec\n",
    "hashing_vec = HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# Fit and transform the Hashing Vectorizer\n",
    "hashed_text = hashing_vec.fit_transform(data.name)\n",
    "\n",
    "# Create DataFrame and print the head\n",
    "hashed_data = pd.DataFrame(hashed_text.data)\n",
    "print(hashed_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punkt Tokenizer\n",
    "\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "\n",
    "trained_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = trained_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
      "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n",
      "[('Tonight', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('comforted', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('hope', 'NN'), ('of', 'IN'), ('a', 'DT'), ('glad', 'JJ'), ('reunion', 'NN'), ('with', 'IN'), ('the', 'DT'), ('husband', 'NN'), ('who', 'WP'), ('was', 'VBD'), ('taken', 'VBN'), ('so', 'RB'), ('long', 'RB'), ('ago', 'RB'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('are', 'VBP'), ('grateful', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('good', 'JJ'), ('life', 'NN'), ('of', 'IN'), ('Coretta', 'NNP'), ('Scott', 'NNP'), ('King', 'NNP'), ('.', '.')]\n",
      "[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n",
      "[('President', 'NNP'), ('George', 'NNP'), ('W.', 'NNP'), ('Bush', 'NNP'), ('reacts', 'VBZ'), ('to', 'TO'), ('applause', 'VB'), ('during', 'IN'), ('his', 'PRP$'), ('State', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Union', 'NNP'), ('Address', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('Capitol', 'NNP'), (',', ','), ('Tuesday', 'NNP'), (',', ','), ('Jan', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking \n",
    "# Making sense of text by grouping in noun, phrases etc.\n",
    "# Chunk gram is verb , noun, adverb, etc.\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            print(chunked)\n",
    "            chunked.draw()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "#process_content()\n",
    "# Chinking - learn later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['filter'] = data['filtered'].apply(lambda x: ' '.join(map(str, x)))\n",
    "#data[['name', 'filter', 'result']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model on filtered data - cap1\n",
    "\n",
    "X = data['filter']\n",
    "y = data['result'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(346955, 140343)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TFIDF is a better method as it offsets a word if it occurs in most documents\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer(min_df=1, stop_words='english')\n",
    "\n",
    "# Convert whole X into cv\n",
    "X_v = tv.fit_transform(X)\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_v, y, random_state=12, stratify=y)\n",
    "\n",
    "X_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(346955,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6609598911677561"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg.score(X_test, y_test) # this accuracy is for data i.e. 100 rows of df. check for all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set 0.740238878470194\n",
      "Accuracy on test set 0.6515869447422727\n"
     ]
    }
   ],
   "source": [
    "# Select model\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Fit training data\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Check training accuracy\n",
    "print('Accuracy on training set ' + str(nb.score(X_train, y_train)))\n",
    "\n",
    "# Predict on test data\n",
    "prediction = nb.predict(X_test)\n",
    "\n",
    "# Check test accuracy\n",
    "print('Accuracy on test set ' + str(nb.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\miniconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'CDF')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# X is a matrix with rows as documents and columns as words. Each entry is 0 or 1 if that words exists in the document or not\n",
    "# Sum each column to collapse in one row which is total number of a given word, sort this list for CDF\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "df = list((X_v > 0).sum(0).tolist()[0])\n",
    "c = pd.Series(df)\n",
    "y = np.arange(c.sum())\n",
    "plt.hist(c, y, normed=True, histtype='step', cumulative=True, linewidth=1.5)\n",
    "\n",
    "plt.xlim(-1, 50)\n",
    "plt.xlabel(\"DF\")\n",
    "plt.ylabel(\"CDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def cv_score(clf, X, y, scorefunc):\n",
    "    result = 0\n",
    "    nfold = 5\n",
    "    for train, test in KFold(nfold).split(X): # split data into train/test groups, 5 times\n",
    "        clf.fit(X[train], y[train]) # fit the classifier, passed is as clf.\n",
    "        result += scorefunc(X[test], y[test]) # evaluate score function on held-out data\n",
    "    return result / nfold # average\n",
    "\n",
    "def log_likelihood(clf, x, y):\n",
    "    prob = clf.predict_log_proba(x)\n",
    "    failed = y == 0\n",
    "    successful = ~failed\n",
    "    return prob[failed, 0].sum() + prob[successful, 1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "_, itest = train_test_split(range(X_v.shape[0]), train_size=0.7)\n",
    "mask = np.zeros(X_v.shape[0], dtype=np.bool)\n",
    "mask[itest] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.6517624710654314\n",
      "aplha: 1\n",
      "min_df: [1, 2, 3, 4, 5, 10, 15, 20, 25, 30]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#the grid of parameters to search over\n",
    "\n",
    "alphas = [0.1, 0.5, 1, 5]\n",
    "min_df = [1, 2, 3, 4, 5, 10, 15, 20, 25, 30] # YOUR TURN: put your value of min_df here.\n",
    "\n",
    "#Find the best value for alpha and min_df, and the best classifier\n",
    "best_min_df = None\n",
    "best_alpha = None\n",
    "maxscore = -np.inf\n",
    "score = None\n",
    "\n",
    "#for min_df in min_df:\n",
    "for alpha in alphas:        \n",
    "        \n",
    "        # Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "        #TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)' \n",
    "        \n",
    "        TOKENS_ALPHANUMERIC = '[^\\d\\W\\_\\-\\:\\;\\!\\@\\#\\%\\&\\*\\?]+' \n",
    "        #vectorizer = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC, ngram_range=(1,2))\n",
    "        \n",
    "        vectorizer = CountVectorizer(ngram_range=(1,1), stop_words=None,\n",
    "                             tokenizer=TreebankWordTokenizer().tokenize)\n",
    "        \n",
    "        # Convert whole X into cv\n",
    "        X_v = vectorizer.fit_transform(X)\n",
    "\n",
    "        Xtrainthis = X_v[mask]\n",
    "        ytrainthis = y[mask]\n",
    "    \n",
    "        # your turn\n",
    "        clf = MultinomialNB(alpha=alpha)\n",
    "        \n",
    "        score = cv_score(clf, Xtrainthis, ytrainthis, clf.score)\n",
    "        \n",
    "        if score > maxscore:\n",
    "            \n",
    "            maxscore = score\n",
    "            best_alpha = alpha\n",
    "            best_min_df = min_df\n",
    "            \n",
    "print('CV score:', maxscore)\n",
    "print('aplha:', best_alpha)\n",
    "print('min_df:', best_min_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.926619\n",
      "Accuracy on test data:     0.659066\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC, ngram_range=(1,2))\n",
    "# Convert whole X into cv\n",
    "X_v = vectorizer.fit_transform(X)\n",
    "xtrain=X_v[mask]\n",
    "ytrain=y[mask]\n",
    "xtest=X_v[~mask]\n",
    "ytest=y[~mask]\n",
    "\n",
    "clf = MultinomialNB(alpha=best_alpha).fit(xtrain, ytrain)\n",
    "\n",
    "#your turn. Print the accuracy on the test and training dataset\n",
    "training_accuracy = clf.score(xtrain, ytrain)\n",
    "test_accuracy = clf.score(xtest, ytest)\n",
    "\n",
    "print(\"Accuracy on training data: {:2f}\".format(training_accuracy))\n",
    "print(\"Accuracy on test data:     {:2f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[129035  21396]\n",
      " [ 61406  31031]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(ytest, clf.predict(xtest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.591890\n",
      "Recall: 0.335699\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "cm = confusion_matrix(ytest, clf.predict(xtest))\n",
    "tp = cm[1, 1]\n",
    "tn = cm[0, 0]\n",
    "fp = cm[0, 1]\n",
    "fn = cm[1, 0]\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "print(\"Precision: {:2f}\".format(precision))\n",
    "print(\"Recall: {:2f}\".format(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', 'a a', 'a ack', ..., 'žít may', 'ƒ', 'ƒ å'], dtype='<U60')"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = np.array(vectorizer.get_feature_names())\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
